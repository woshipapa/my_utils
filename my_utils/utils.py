import torch
import sys
import torch.distributed as dist
import time

# from megatron.core.tensor_parallel.mappings import (
#     gather_from_tensor_model_parallel_region,
# )
import hashlib
import torch.nn as nn

# from .logging import get_logger
# from .logger import get_logger

from contextlib import contextmanager



def print_model_params(model):
    print("Model Parameters:")
    print("=" * 50)
    for name, param in model.named_parameters():
        if isinstance(param, torch.Tensor):
            print(f"Layer: {name}")
            print(f"Shape: {param.shape}")
            print(param.data)  # ä»…æ‰“å°æ•°å€¼ï¼Œä¸è®¡ç®—æ¢¯åº¦
            print("-" * 50)


def tensor_md5(tensor: torch.Tensor) -> str:
    tensor = tensor.to(torch.float64)
    # ç¡®ä¿ Tensor åœ¨ CPU ä¸Šï¼Œå¹¶è½¬æ¢ä¸º numpy æ•°ç»„
    tensor_np = tensor.detach().cpu().numpy()
    # å°† numpy æ•°ç»„è½¬æ¢ä¸º bytes
    tensor_bytes = tensor_np.tobytes()
    # è®¡ç®— MD5
    md5_hash = hashlib.md5(tensor_bytes).hexdigest()
    return md5_hash


class DebugLayer(nn.Module):
    def __init__(self):
        super().__init__()

    def forward(self, *args):
        if len(args) == 1:
            return args[0]
        return args


filename = "_output_14_backward_2.log"


def register_hooks(model, print_values=True, max_elements=20):
    hooks = []

    def print_shape_and_values(tensor, label, max_elements=20):
        """Helper function to print shape and values of a tensor"""
        rank = 0
        with open(str(rank) + filename, "a") as f:
            if isinstance(tensor, torch.Tensor):
                get_logger().info(f"[rank {rank}] {label} shape: {tensor.shape}")
                if print_values:
                    if tensor.numel() < max_elements:
                        max_elements = tensor.numel()
                    if tensor.flatten()[0].dtype != torch.bool:
                        # get_logger().info(f"[rank {rank}]  {label} first 20 values: {tensor.flatten()[:max_elements]}{'...' if tensor.numel() > max_elements else ''} ")
                        # x, _ = torch.topk(tensor.flatten(), max_elements)
                        # get_logger().info(f"[rank {rank}]  {label} max 20 values: {x} ")
                        # x, _ = torch.topk(-tensor.flatten(), max_elements)
                        # get_logger().info(f"[rank {rank}]  {label} min 20 values: {x} ")
                        tensor = tensor.float()
                        x = torch.norm(tensor)
                        get_logger().info(f"[rank {rank}] {label} norm Values: {x} ")
                        # get_logger().info(f'[rank {rank}] {label} md5 value: {tensor_md5(tensor)} ')
                        get_logger().info(
                            f"[rank {rank}] {label} shape: {tensor.shape} "
                        )
            else:
                get_logger().info(f"[rank {rank}]  {label} is not tensor, is {tensor} ")

    def forward_hook_fn(module, input, output, name):
        """Hook å‡½æ•°ï¼Œæ‰“å°è¾“å…¥å’Œè¾“å‡ºçš„ shape å’Œå…·ä½“æ•°å€¼"""
        # print(f"Layer: {module.__class__.__name__}")
        rank = 0
        with open(str(rank) + filename, "a") as f:
            get_logger().info(f"[rank {rank}] Layer: {name}")
            if hasattr(module, "weight") and module.weight is not None:
                weight = module.weight.data
                print_shape_and_values(weight, f"{name}_Weight")
            else:
                get_logger().info(f"[rank {rank}] Layer: {name} has no weight ")
        # æ‰“å°è¾“å…¥
        if isinstance(input, tuple):
            for idx, inp in enumerate(input):
                if isinstance(inp, tuple):  # Check for nested tuple
                    for sub_idx, sub_inp in enumerate(inp):
                        print_shape_and_values(sub_inp, f"Input {idx}-{sub_idx}")
                else:
                    print_shape_and_values(inp, f"{name}_Input {idx}")
        else:
            print_shape_and_values(input, "Input")

        # Print output shapes and values
        if isinstance(output, tuple):
            for i, tensor in enumerate(output):
                if isinstance(tensor, tuple):  # Check for nested tuple
                    for sub_idx, sub_tensor in enumerate(tensor):
                        print_shape_and_values(
                            sub_tensor, f"{name}_Output {i}-{sub_idx}"
                        )
                else:
                    print_shape_and_values(tensor, f"{name}_Output {i}")

        else:
            print_shape_and_values(output, "Output")
        with open(str(rank) + filename, "a") as f:
            get_logger().info("-" * 100)
            get_logger().info(" ")

    def backward_hook_fn(module, grad_input, grad_output, name):
        """Hook å‡½æ•°ï¼Œæ‰“å°åå‘ä¼ æ’­æ—¶çš„æ¢¯åº¦"""
        # print(f"Layer: {module.__class__.__name__} (backward)")
        rank = 0
        with open(str(rank) + filename, "a") as f:
            get_logger().info(f"[rank {rank}] Layer: {name} (backward)")
            # æ‰“å°è¾“å…¥æ¢¯åº¦
            for idx, grad in enumerate(grad_input):
                if grad is not None:
                    print_shape_and_values(grad, f"{name}_Grad Input {idx}")

            # æ‰“å°è¾“å‡ºæ¢¯åº¦
            for idx, grad in enumerate(grad_output):
                if grad is not None:
                    print_shape_and_values(grad, f"{name}_Grad Output {idx}")

            # if hasattr(module, 'weight') and module.weight is not None:
            #     print_shape_and_values(module.weight.grad, f"{name}_weight_grad")
            # if hasattr(module, 'bias') and module.bias is not None:
            #     print_shape_and_values(module.bias.grad, f"{name}_bias_grad")

            get_logger().info("-" * 100)
            get_logger().info(" ")

    # éå†æ¨¡å‹çš„æ‰€æœ‰å±‚å¹¶æ³¨å†Œ hook
    # for layer in model.modules():
    #     if not isinstance(layer, torch.nn.Sequential) and not isinstance(layer, torch.nn.ModuleList):
    #         forward_hook = layer.register_forward_hook(forward_hook_fn)
    #         backward_hook = layer.register_backward_hook(backward_hook_fn)
    #         hooks.append(forward_hook)
    #         hooks.append(backward_hook)

    # if dist.is_available() and dist.is_initialized():
    #     rank = 0
    # else:
    #     rank = 0

    def watch_parameter(param_name, param):
        rank = 0

        def param_hook(grad):
            if grad is not None:
                with open(str(rank) + filename + "_grad", "a") as f:
                    get_logger().info(f"{param_name} grad norm: {grad.norm()} ")
            else:
                with open(str(rank) + filename + "_grad", "a") as f:
                    get_logger().info(f"{param_name} grad is None ")

        param.register_hook(param_hook)

    for name, module in model.named_modules():
        forward_hook = module.register_forward_hook(
            lambda m, i, o, name=name: forward_hook_fn(m, i, o, name)
        )
        backward_hook = module.register_full_backward_hook(
            lambda m, i, o, name=name: backward_hook_fn(m, i, o, name)
        )
        hooks.append(forward_hook)
        hooks.append(backward_hook)

    for name, param in model.named_parameters():
        if param.requires_grad:
            watch_parameter(name, param)

    return hooks  # è¿”å› hook å¥æŸ„åˆ—è¡¨ï¼Œæ–¹ä¾¿åç»­æ¸…ç†


import time, os, re
from collections import defaultdict
import numpy as np
import logging
# from t2v_flow.executor.DynamicForwardStepHandler import DynamicForwardStepHandler
from logging import LoggerAdapter

try:
    # nvidia nvtx not torch.cuda.nvtx
    import nvtx
    NVTX_AVAILABLE = True
except ImportError:
    # ... (dummy nvtx class)
    NVTX_AVAILABLE = False
    class nvtx:
        @staticmethod
        def start_range(*args, **kwargs): return None
        @staticmethod
        def end_range(*args, **kwargs): pass
try:
    import psutil
    PSUTIL_AVAILABLE = True
except ImportError:
    PSUTIL_AVAILABLE = False

class MyTimer:
    # __init__, start, stop, next_iteration, _gather_records, summarize, summarize_per_rank, dump ç­‰æ–¹æ³•ä¿æŒä¸å˜...
    # (æ­¤å¤„çœç•¥äº†ä¹‹å‰å·²å±•ç¤ºçš„ã€æœªæ”¹åŠ¨çš„æ–¹æ³•ä»£ç ï¼Œä»¥ä¿æŒç®€æ´)
    def __init__(self, use_cuda=True, tag="timer", 
                 verbose=True, log_dir="my_timer_log/",
                 profile_memory=False, use_nvtx=False):
        self.use_cuda = use_cuda and torch.cuda.is_available()
        self.verbose = verbose
        self.tag = tag
        self.rank = dist.get_rank() if dist.is_initialized() else 0
        self.world_size = dist.get_world_size() if dist.is_initialized() else 1
        self.log_dir = log_dir
        
        self.records = []
        self.current_iteration = 0
        
        self.profile_memory = profile_memory and PSUTIL_AVAILABLE and self.use_cuda
        self.use_nvtx = use_nvtx and NVTX_AVAILABLE
        self.log_context = {'log_type': 'timing'}
        # self.logger = self._get_logger()

        # 1. ç­‰å¾…å¤–éƒ¨çš„é…ç½®å‚æ•°ä¼ å…¥æˆ–è€…2. è‡ªå·±å»ä¸»åŠ¨å¯»æ‰¾ï¼ˆè€¦åˆç‰ˆï¼‰
        self.logger = None
        # if verbose: self.logger.setLevel(logging.INFO)
        # else: self.logger.setLevel(logging.WARNING)

        # nvidia nvtx 
        self._domains = {}
        self._registered_attrs = {}


        # async
        # æ–°å¢ï¼šç”¨äºå­˜æ”¾æœ¬è½®è¿­ä»£ä¸­å·²å®Œæˆä½†å°šæœªè®¡ç®—æ—¶é—´çš„è®°å½•
        self._pending_records = []

        # self._stage_times = {}

        # --- [!!] æ–°çš„å±‚çº§å †æ ˆé€»è¾‘ [!!] ---
        
        # 1. (æ›¿æ¢) ç§»é™¤ self._stage_times = {}
        # 2. (æ–°å¢) æˆ‘ä»¬éœ€è¦ä¸€ä¸ªå”¯ä¸€çš„èŠ‚ç‚¹ ID æ¥åœ¨
        #    'summarize' é˜¶æ®µé‡å»ºçˆ¶å­å…³ç³»ã€‚
        self.next_node_id = 1 # 0 æ˜¯ root
        
        # 3. (æ–°å¢) åˆ›å»ºæ ¹èŠ‚ç‚¹ (root_node)ã€‚
        #    current_node å§‹ç»ˆæŒ‡å‘å †æ ˆçš„é¡¶éƒ¨ã€‚
        self.root_node = {
            "name": "root",
            "node_id": 0,
            "parent_id": None,
            "start_cpu": time.perf_counter(),
            "children": [] # (ç”¨äºè°ƒè¯•, ä¸»è¦æ•°æ®åœ¨ records ä¸­)
        }

        self.current_node = self.root_node

    def _get_domain(self, domain_name: str):
        """å†…éƒ¨æ–¹æ³•ï¼Œç”¨äºè·å–å¹¶ç¼“å­˜ Domain å¯¹è±¡"""
        if domain_name is None:
            return None
        if domain_name not in self._domains:
            # å¦‚æœåŸŸä¸å­˜åœ¨ï¼Œåˆ™åˆ›å»ºä¸€ä¸ªæ–°çš„ Domain å¯¹è±¡å¹¶ç¼“å­˜
            self._domains[domain_name] = nvtx.get_domain(domain_name)

        return self._domains[domain_name]

    def register_stage(self, stage_name: str, color: str = "blue", domain_name: str = None, category = None):
        if not self.use_nvtx:
            return
            
        if domain_name is None:
            raise ValueError("Pre-registration for NVTX optimization requires a valid `domain_name`.")
            
        domain = self._get_domain(domain_name)
        
        # nvtx.Domainçš„æ–¹æ³• get_event_attributes
        attrs = domain.get_event_attributes(
            message=stage_name, color=color, category=category
        )
        
        self._registered_attrs[stage_name] = (domain, attrs)

    def disable_cuda_time(self):
        self.use_cuda = False

    def set_logger(self, logger_instance: logging.Logger):
        """
        ã€å…¬å…±æ¥å£ã€‘å…è®¸å¤–éƒ¨é¡¹ç›®æ³¨å…¥è‡ªå·±çš„ logger å®ä¾‹ã€‚
        æ³¨å…¥åä¼šç«‹å³ç”¨ LoggerAdapter åŒ…è£…ï¼Œä»¥æ”¯æŒè¿‡æ»¤å™¨ã€‚
        """
        # å³ä½¿å¤–éƒ¨æ³¨å…¥ï¼Œä¹Ÿç”¨ Adapter åŒ…è£…ä»¥ç¡®ä¿ log_context å­˜åœ¨
        # logger_instanceæ˜¯global_logger.get_logger()è¿”å›çš„logger
        self.logger = LoggerAdapter(logger_instance, self.log_context)
        

        # å•ä¾‹
        global_logger = GlobalLogger()

        setattr(self.logger, 'log_profile_event', global_logger.log_profile_event)

    def _create_default_logger(self) -> logging.Logger:
        """
        ã€å…¨æ–°ã€‘æŒ‰æ‚¨ handler é¡¹ç›®çš„æ ·å¼ï¼Œåˆ›å»ºä¸€ä¸ªåŠŸèƒ½å®Œå¤‡çš„é»˜è®¤ loggerã€‚
        """
        # ç¡®ä¿æ—¥å¿—ç›®å½•å­˜åœ¨
        os.makedirs(self.log_dir, exist_ok=True)
        
        logger = logging.getLogger(f"MyTimer.default_rank_{self.rank}")
        
        # å¦‚æœå·²ç»é…ç½®è¿‡ï¼Œåˆ™ç›´æ¥è¿”å›ï¼Œé˜²æ­¢é‡å¤æ·»åŠ  handler
        if logger.handlers:
            return logger
            
        logger.setLevel(logging.INFO if self.verbose else logging.WARNING)
        logger.propagate = False

        formatter = logging.Formatter(
            f"[%(asctime)s] [Rank {self.rank}] [%(levelname)s] [%(funcName)s:%(lineno)d] - %(message)s",
            datefmt="%Y-%m-%d %H:%M:%S",
        )

        # 1. é…ç½®æ§åˆ¶å° Handler
        stream_handler = logging.StreamHandler(sys.stdout)
        stream_handler.setFormatter(formatter)
        logger.addHandler(stream_handler)

        # 2. é…ç½®æ–‡ä»¶ Handler (æ‰€æœ‰æ—¥å¿—)
        main_log_file = os.path.join(self.log_dir, f"timer_rank_{self.rank}.log")
        main_file_handler = logging.FileHandler(main_log_file, mode="a")
        main_file_handler.setFormatter(formatter)
        logger.addHandler(main_file_handler)
        
        return logger

    # def _get_logger(self):
    #     handler = DynamicForwardStepHandler()
    #     if handler.logger is None:
    #         return None
    #     adapter = LoggerAdapter(handler.logger, self.log_context)
    #     return adapter


    def _ensure_logger(self):
        """
        ã€å…¨æ–°æ ¸å¿ƒé€»è¾‘ã€‘å³æ—¶è§£æ Loggerï¼Œè§£å†³åˆå§‹åŒ–æ—¶åºé—®é¢˜ã€‚
        """
        # å¦‚æœ logger å·²ç»è¢«å¤–éƒ¨é€šè¿‡ set_logger æ³¨å…¥ï¼Œåˆ™ä»€ä¹ˆéƒ½ä¸åš
        if self.logger:
            return

        # 1. ä¼˜å…ˆå°è¯•è·å– handler é¡¹ç›®çš„ logger
        raw_logger = None
        try:
            from t2v_flow.executor.DynamicForwardStepHandler import DynamicForwardStepHandler
            handler = DynamicForwardStepHandler()
            if handler.logger:
                raw_logger = handler.logger
        except (ImportError, AttributeError):
            # å¯¼å…¥å¤±è´¥æˆ–å±æ€§ä¸å­˜åœ¨ï¼Œè¯´æ˜ä¸åœ¨ handler é¡¹ç›®ä¸­ï¼Œæ­£å¸¸ç°è±¡
            pass

        # 2. å¦‚æœæ²¡è·å–åˆ°ï¼Œåˆ™åˆ›å»ºç³»ç»Ÿåˆå§‹åŒ–æ—¶åˆ›å»ºçš„é»˜è®¤ global logger
        if raw_logger is None:
            from my_utils.logger import GlobalLogger
            raw_logger = GlobalLogger().get_logger()
            
        # 3. æ— è®ºæ¥æºå¦‚ä½•ï¼Œéƒ½ç”¨ LoggerAdapter åŒ…è£…ä»¥æ·»åŠ ä¸Šä¸‹æ–‡ï¼Œä½¿è¿‡æ»¤å™¨ç”Ÿæ•ˆ
        self.logger = LoggerAdapter(raw_logger, self.log_context)

    @contextmanager
    def time_stage(self, stage_name: str):
        """
        ä¸€ä¸ªä¸Šä¸‹æ–‡ç®¡ç†å™¨ï¼Œç”¨äºæ–¹ä¾¿ã€å®‰å…¨åœ°å¯¹ä»£ç å—è¿›è¡Œè®¡æ—¶ã€‚

        Args:
            stage_name (str): è¦è®¡æ—¶çš„é˜¶æ®µæˆ–ä»£ç å—çš„åç§°ã€‚
        
        Usage:
            timer = MyTimer()
            with timer.time_stage('data_loading'):
                # your code to be timed
                time.sleep(1)
        """
        self.start(stage_name)
        try:
            yield
        finally:
            self.stop(stage_name)

    def start(self, stage_name: str, 
              color: str = "blue",
              domain_name: str = None):
        # if self.logger is None:
        #     self.logger = self._get_logger()
        # æ­¤å¤„è€¦åˆäº†handlerçš„loggeråœ¨è¿™é‡Œè®¾å®šï¼Œç›®å‰å·²ç»è¿ç§»åˆ°åˆå§‹åŒ–å®Œmegatronåˆ†å¸ƒå¼ç¯å¢ƒåä¼ å…¥logger
        self._ensure_logger()
        if torch.distributed.is_initialized() and self.rank != dist.get_rank():
            self.rank = dist.get_rank()
        # entry = {"cpu_start": time.time()}
        # CPU code start time
        entry = {"cpu_start": time.perf_counter()}
        
        # GPU profile using cudaEvent
        if self.use_cuda:
            entry["cuda_start"] = torch.cuda.Event(enable_timing=True)
            entry["cuda_end"] = torch.cuda.Event(enable_timing=True)
            entry["cuda_start"].record()



        if self.use_nvtx:
            entry["nvtx_domain"] = None  # ç”¨äºåœ¨ stop æ—¶åˆ¤æ–­è°ƒç”¨å“ªä¸ª end_range
            entry["nvtx_range_id"] = None

            if stage_name in self._registered_attrs:
                # ä¼˜åŒ–è·¯å¾„: ä½¿ç”¨é¢„ç¼“å­˜çš„ (domain, attrs)
                domain, attrs = self._registered_attrs[stage_name]
                entry["nvtx_domain"] = domain
                entry["nvtx_range_id"] = domain.start_range(attributes=attrs)
            else:
                # æ¢ç´¢è·¯å¾„: å³æ—¶åˆ›å»º
                entry["nvtx_range_id"] = nvtx.start_range(
                    message=stage_name, color=color, domain=domain_name
                )
                # æ­¤æ—¶ entry["nvtx_domain"] ä¿æŒä¸º Noneï¼Œä½œä¸ºä½¿ç”¨å…¨å±€å‡½æ•°çš„æ ‡è®°


        new_node_id = self.next_node_id
        self.next_node_id += 1
        
        new_node = {
            "name": stage_name,
            "node_id": new_node_id,
            "parent_id": self.current_node["node_id"],
            "children": [],
            # (ç”¨äº stop() æ–¹æ³•çš„å †æ ˆä¸Šæµ®)
            "parent": self.current_node,
            "abs_start_time": time.time(),
            # (å­˜å‚¨ä½  'entry' å­—å…¸ä¸­çš„æ‰€æœ‰å†…å®¹)
            "cpu_start": entry["cpu_start"],
            "cuda_start": entry.get("cuda_start"),
            "cuda_end":  entry.get("cuda_end"),
            "nvtx_domain": entry.get("nvtx_domain"),
            "nvtx_range_id": entry.get("nvtx_range_id"),
        }

        self.current_node["children"].append(new_node)
        self.current_node = new_node
        # self._stage_times[stage_name] = entry

    # def stop(self, stage_name):
    #     if stage_name not in self._stage_times:
    #         return
    #     entry = self._stage_times.pop(stage_name)
    #     # cpu_end = time.time()
    #     cpu_end = time.perf_counter()
    #     cpu_elapsed_ms = (cpu_end - entry.get("cpu_start", cpu_end)) * 1000
    #     cuda_elapsed_ms = None
    #     if self.use_cuda and "cuda_end" in entry:
    #         entry["cuda_end"].record()
    #         # torch.cuda.synchronize()
    #         # cuda_elapsed_ms = entry["cuda_start"].elapsed_time(entry["cuda_end"])

    #     if self.use_nvtx and "nvtx_range_id" in entry:
    #         domain = entry.get("nvtx_domain")
    #         range_id = entry.get("nvtx_range_id")

    #         if range_id is not None:
    #             if domain:
    #                 # å¦‚æœ start æ˜¯åœ¨ domain å¯¹è±¡ä¸Šè°ƒç”¨çš„ï¼Œend ä¹Ÿå¿…é¡»åœ¨åŒä¸€ä¸ªå¯¹è±¡ä¸Šè°ƒç”¨
    #                 domain.end_range(range_id)
    #             else:
    #                 # å¦‚æœ start æ˜¯ç”¨å…¨å±€å‡½æ•°è°ƒç”¨çš„ï¼Œend ä¹Ÿå¿…é¡»ç”¨å…¨å±€å‡½æ•°
    #                 nvtx.end_range(range_id)



    #      # æš‚å­˜è®°å½•ï¼ŒCPU æ—¶é—´å·²çŸ¥ï¼ŒGPU äº‹ä»¶å·²è®°å½•ä½†æ—¶é—´æœªçŸ¥
    #     pending_record = {
    #         "stage": stage_name,
    #         "cpu_duration_ms": (cpu_end - entry["cpu_start"]) * 1000,
    #         "cuda_events": (entry.get("cuda_start"), entry.get("cuda_end"))
    #     }
    #     self._pending_records.append(pending_record)    
    #     # self.records.append(
    #     #     {
    #     #         "stage": stage_name,
    #     #         "rank": self.rank,
    #     #         "iteration": self.current_iteration,
    #     #         "cpu_duration_ms": cpu_elapsed_ms,
    #     #         "cuda_duration_ms": cuda_elapsed_ms,
    #     #     }
    #     # )

    #     # # ä½¿ç”¨ä¼ å…¥çš„loggeræ¥è®°å½•timerä¸­çš„æ•°æ®ä¿¡æ¯
    #     # if self.verbose:
    #     #     self.logger.info(
    #     #         f"[Iter {self.current_iteration}] Stage '{stage_name}': CPU {cpu_elapsed_ms:.3f}ms, CUDA {cuda_elapsed_ms or 0.0:.3f}ms"
    #     #     )
    def stop(self, stage_name: str):
        cpu_end = time.perf_counter()

        # [!!] (V2) 1. å †æ ˆæ£€æŸ¥ (æœ€å…³é”®çš„éƒ¨åˆ†) [!!]
        if self.current_node["name"] != stage_name:
            # (å¥å£®æ€§å¤„ç†ï¼šå¦‚æœåç§°ä¸åŒ¹é…, æˆ‘ä»¬å°è¯•åœ¨å †æ ˆä¸­å‘ä¸ŠæŸ¥æ‰¾)
            # (è¿™å¯ä»¥å¤„ç† "stop('A')" è‡ªåŠ¨å…³é—­ "B" çš„æƒ…å†µ)
            
            print(f"TimerWarning: Mismatched stop call on Rank {self.rank}! "
                  f"Expected to stop '{self.current_node['name']}' but got '{stage_name}'.")
            
            node_to_stop = self._find_node_in_stack(stage_name)
            
            if node_to_stop is None:
                print(f"TimerError: Could not find active timer '{stage_name}' in the stack.")
                return

            # å¦‚æœæ‰¾åˆ°äº†, æˆ‘ä»¬å¿…é¡»è‡ªåŠ¨å…³é—­æ‰€æœ‰å­èŠ‚ç‚¹, ç›´åˆ°æˆ‘ä»¬åˆ°è¾¾
            # 'node_to_stop'ã€‚
            while self.current_node != node_to_stop:
                print(f"TimerWarning: Auto-stopping child '{self.current_node['name']}' "
                      f"due to explicit stop of ancestor '{stage_name}'.")
                # (ä¼ å…¥ cpu_end, å› ä¸ºè¿™æ˜¯å”¯ä¸€çš„ "stop" æ—¶é—´)
                self._finalize_and_record_node(self.current_node, cpu_end) 
                self.current_node = self.current_node['parent']
            
        # [!!] (V2) 2. æœ€ç»ˆç¡®å®šå¹¶è®°å½•å½“å‰èŠ‚ç‚¹
        self._finalize_and_record_node(self.current_node, cpu_end)

        # [!!] (V2) 3. ä¸Šæµ® (Ascend)
        # (æˆ‘ä»¬ *æ€»æ˜¯* ä¸Šæµ®åˆ°å½“å‰èŠ‚ç‚¹çš„çˆ¶èŠ‚ç‚¹)
        if self.current_node["parent"] is not None:
            self.current_node = self.current_node["parent"]
        else:
            print(f"TimerError: Attempted to stop root node?")


    def _find_node_in_stack(self, name: str):
        """ (æ–°å¢) è¾…åŠ©å‡½æ•°: ä»å½“å‰èŠ‚ç‚¹å‘ä¸Šæœç´¢å †æ ˆ """
        temp_node = self.current_node
        while temp_node is not None and temp_node["name"] != "root":
            if temp_node["name"] == name:
                return temp_node
            temp_node = temp_node["parent"]
        return None
    
    def _finalize_and_record_node(self, node: dict, cpu_end: float):
        """ (æ–°å¢) è¾…åŠ©å‡½æ•°: åŒ…å«ä½  'stop' æ–¹æ³•çš„æ‰€æœ‰æ ¸å¿ƒé€»è¾‘ """
        
        # [!!] (V2) è¿™éƒ¨åˆ†ä»£ç  *å°±æ˜¯* ä½  'stop' æ–¹æ³•çš„ 90% [!!]
        
        # (æ¥è‡ªä½  'stop' çš„é€»è¾‘)
        if self.use_cuda and "cuda_end" in node and node["cuda_end"] is not None:
            node["cuda_end"].record()

        if self.use_nvtx and "nvtx_range_id" in node:
            domain = node.get("nvtx_domain")
            range_id = node.get("nvtx_range_id")
            if range_id is not None:
                if domain:
                    domain.end_range(range_id)
                else:
                    nvtx.end_range(range_id)

        # (æ¥è‡ªä½  'stop' çš„é€»è¾‘: åˆ›å»º pending_record)
        pending_record = {
            "stage": node["name"],
            "cpu_duration_ms": (cpu_end - node["cpu_start"]) * 1000,
            "cuda_events": (node.get("cuda_start"), node.get("cuda_end")),
            "abs_start_time": node.get("abs_start_time", 0.0),
            # [!!] (V2) æ–°å¢çš„å±‚çº§æ•°æ® [!!]
            # (è¿™å…è®¸ 'summarize' é‡å»ºæ ‘)
            "node_id": node["node_id"],
            "parent_id": node["parent_id"]
        }
        self._pending_records.append(pending_record)

    def set_step(self, iteration: int):
        self.current_iteration = iteration
    def synchronize_and_log(self):
        """
        [V2 - å±‚çº§ç‰ˆæœ¬]
        åœ¨è¿­ä»£ç»“æŸæ—¶è°ƒç”¨ï¼Œæ‰§è¡ŒåŒæ­¥ï¼Œå¹¶å®Œæˆä¸‰ä»¶äº‹ï¼š
        1. è®¡ç®—æ‰€æœ‰ pending records çš„ CUDA æ—¶é—´å¹¶å­˜å…¥ self.recordsã€‚
        2. ä» self.records (æ‰å¹³åˆ—è¡¨) é‡å»ºæœ¬æ¬¡è¿­ä»£çš„è°ƒç”¨æ ‘ã€‚
        3. éå†è¯¥æ ‘ï¼Œè®¡ç®— Self Time å¹¶ä»¥å±‚çº§ï¼ˆç¼©è¿›ï¼‰æ ¼å¼è®°å½•æ—¥å¿—ã€‚
        """
        if self.use_cuda:
            torch.cuda.synchronize()

        # --- æ­¥éª¤ 1: å¤„ç† _pending_records å¹¶å¡«å…… self.records ---
        
        # æ¸…ç©ºæœ¬æ¬¡è¿­ä»£çš„ records
        self.records = []
        
        for record in self._pending_records:
            cuda_elapsed_ms = None
            if self.use_cuda:
                start_event, end_event = record["cuda_events"]
                if start_event and end_event:
                    # ç¡®ä¿äº‹ä»¶å·²å‡†å¤‡å¥½
                    try:
                        cuda_elapsed_ms = start_event.elapsed_time(end_event)
                    except torch.cuda.Error as e:
                        # (å¤„ç†å¯èƒ½çš„ CUDA é”™è¯¯)
                        self.logger.warning(f"CUDA event error for {record['stage']}: {e}")
            
            abs_start_ts = record.get("abs_start_time", 0.0)
            
            # B. ç¡®å®šè€—æ—¶ (ä¼˜å…ˆç”¨ CUDA è€—æ—¶ï¼Œå¦‚æœæ˜¯çº¯ CPU æ“ä½œåˆ™ç”¨ CPU è€—æ—¶)
            final_duration_ms = cuda_elapsed_ms if cuda_elapsed_ms is not None else record["cpu_duration_ms"]
            
            # C. æ¨ç®—ç»“æŸæ—¶é—´ç‚¹ (Start + Duration)
            # æ³¨æ„ ms è½¬ s
            abs_end_ts = abs_start_ts + (final_duration_ms / 1000.0)

            self.logger.log_profile_event(
                timestamp=abs_start_ts,
                step=self.current_iteration,
                event_name=record["stage"],
                event_type="START",
                metadata=f"node_id={record['node_id']}"
            )
            
            # E. å†™å…¥ END äº‹ä»¶
            self.logger.log_profile_event(
                timestamp=abs_end_ts,
                step=self.current_iteration,
                event_name=record["stage"],
                event_type="END",
                duration_ms=final_duration_ms,
                metadata=f"node_id={record['node_id']}" # å¯ä»¥è®°å½•æ›´å¤š meta
            )
            full_record = {
                "stage": record["stage"],
                "rank": self.rank,
                "iteration": self.current_iteration,
                "cpu_duration_ms": record["cpu_duration_ms"],
                "cuda_duration_ms": cuda_elapsed_ms,
                
                # [!!] å…³é”®: ä¿ç•™ V2 å †æ ˆ timer æä¾›çš„å±‚çº§ ID
                "node_id": record["node_id"],
                "parent_id": record["parent_id"],
                
                # (ç”¨äºæ­¥éª¤ 2 çš„ä¸´æ—¶å­—æ®µ)
                "children": []
            }
            self.records.append(full_record)
        
        self._pending_records.clear()

        if not self.records or not self.verbose:
            # å¦‚æœæ²¡æœ‰è®°å½•, æˆ–è€…æˆ‘ä»¬å¤„äºé verbose æ¨¡å¼, å°±æå‰é€€å‡º
            # (self.records ä»ç„¶è¢«å¡«å……, åªæ˜¯ä¸è®°å½•æ—¥å¿—)
            return

        # --- æ­¥éª¤ 2: ä» self.records (æ‰å¹³åˆ—è¡¨) é‡å»ºè°ƒç”¨æ ‘ ---
        
        # ä½¿ç”¨å­—å…¸ä»¥ä¾¿å¿«é€ŸæŸ¥æ‰¾
        nodes_map = {node['node_id']: node for node in self.records}
        
        # (æ³¨æ„: æˆ‘ä»¬å‡è®¾ self.root_node (id=0) æ˜¯å…¨å±€æ ¹,
        #  æˆ‘ä»¬åœ¨è¿™é‡Œåªæ„å»ºæœ¬æ¬¡è¿­ä»£çš„å­æ ‘)
        
        tree_roots = [] # å­˜æ”¾æœ¬æ¬¡è¿­ä»£çš„"é¡¶å±‚"è°ƒç”¨
        
        for node in self.records:
            parent_id = node['parent_id']
            if parent_id in nodes_map:
                # è¿™æ˜¯ä¸€ä¸ªå­èŠ‚ç‚¹, å°†å®ƒæ·»åŠ åˆ°å…¶çˆ¶èŠ‚ç‚¹çš„ 'children' åˆ—è¡¨ä¸­
                parent_node = nodes_map[parent_id]
                parent_node['children'].append(node)
            else:
                # è¿™æ˜¯ä¸€ä¸ªé¡¶å±‚èŠ‚ç‚¹ (å…¶çˆ¶èŠ‚ç‚¹ä¸æ˜¯æœ¬æ¬¡è¿­ä»£çš„è®°å½•, 
                # å¯èƒ½æ˜¯å…¨å±€æ ¹èŠ‚ç‚¹ 'id=0')
                tree_roots.append(node)

        # --- æ­¥éª¤ 3: é€’å½’è®¡ç®— Self Time ---
        
        def calculate_self_time_recursive(node):
            """
            éå†æ ‘, è®¡ç®—æ¯ä¸ªèŠ‚ç‚¹çš„ Self Timeã€‚
            è¿”å›: è¯¥èŠ‚ç‚¹çš„ Total CUDA Time (ç”¨äºçˆ¶èŠ‚ç‚¹çš„è®¡ç®—)ã€‚
            """
            # æˆ‘ä»¬ä½¿ç”¨ CUDA æ—¶é—´ä½œä¸º "Total Time"
            total_time_ms = node.get('cuda_duration_ms') or 0.0
            
            if not node['children']:
                # å¦‚æœæ˜¯å¶èŠ‚ç‚¹, Self Time == Total Time
                node['self_time_ms'] = total_time_ms
                return total_time_ms
                
            # é€’å½’è®¡ç®—æ‰€æœ‰å­èŠ‚ç‚¹çš„æ€»æ—¶é—´
            children_total_time = 0.0
            for child in node['children']:
                children_total_time += calculate_self_time_recursive(child)
            
            # Self Time = Total Time - Children's Total Time
            node['self_time_ms'] = total_time_ms - children_total_time
            
            # è¿”å› *Total Time* ç»™çˆ¶èŠ‚ç‚¹
            return total_time_ms

        # éå†æ‰€æœ‰é¡¶å±‚èŠ‚ç‚¹æ¥å¯åŠ¨è®¡ç®—
        for root_node in tree_roots:
            calculate_self_time_recursive(root_node)

        # --- æ­¥éª¤ 4: é€’å½’åœ°è®°å½•å±‚çº§æ—¥å¿— ---
        
        def log_tree_recursive(node, indent_prefix=""):
            """
            éå†æ ‘, å¹¶ä»¥ç¼©è¿›æ ¼å¼æ‰“å°æ—¥å¿—ã€‚
            """
            # å‡†å¤‡æ—¥å¿—æ¡ç›®
            stage = node['stage']
            cpu_ms = node['cpu_duration_ms']
            cuda_ms = node.get('cuda_duration_ms') or 0.0
            self_ms = node.get('self_time_ms', 0.0) # self_time æ˜¯æˆ‘ä»¬åˆšè®¡ç®—çš„
            
            # [!!] è¿™å°±æ˜¯ä½ çš„ "Log Parsing" è§£å†³æ–¹æ¡ˆ [!!]
            node_id = node['node_id']
            parent_id = node['parent_id']
            
            # æ ¼å¼åŒ– Self Time (ä»…åœ¨æœ‰å­èŠ‚ç‚¹ä¸” Self > 0 æ—¶æ˜¾ç¤º)
            self_time_str = ""
            if node['children'] and self_ms > 0.001:
                self_time_str = f", Self {self_ms:.3f}ms"

            # [!!] æœ€ç»ˆçš„ã€å¯è§£æçš„ã€å±‚çº§çš„æ—¥å¿—æ¶ˆæ¯ [!!]
            log_msg = (
                f"[Iter {self.current_iteration}] {indent_prefix}"
                f"Stage '{stage}' [id={node_id}, p_id={parent_id}]: "
                f"CPU {cpu_ms:.3f}ms, CUDA {cuda_ms:.3f}ms{self_time_str}"
            )
            
            self.logger.info(log_msg)
            
            # é€’å½’åœ°æ‰“å°å­èŠ‚ç‚¹
            new_indent = indent_prefix + "  L "
            
            # (æŒ‰ node_id æ’åº, ä»¥ç¡®ä¿æ—¥å¿—é¡ºåºä¸è°ƒç”¨é¡ºåºå¤§è‡´åŒ¹é…)
            sorted_children = sorted(node['children'], key=lambda x: x['node_id'])
            
            for child in sorted_children:
                log_tree_recursive(child, indent_prefix=new_indent)

        # ç¡®ä¿ self.logger å¯ç”¨
        self._ensure_logger()

        # (æŒ‰ node_id æ’åºæ ¹èŠ‚ç‚¹)
        sorted_roots = sorted(tree_roots, key=lambda x: x['node_id'])
        
        # å¯åŠ¨æ—¥å¿—è®°å½•
        for root_node in sorted_roots:
            log_tree_recursive(root_node, indent_prefix="") # é¡¶å±‚æ²¡æœ‰ç¼©è¿›

    def step(self):
        self.synchronize_and_log()

        
    def next_iteration(self):
        self.current_iteration += 1
        self.logger.info(
            f"--- MyTimer: Switched to iteration {self.current_iteration} ---"
        )

    def _gather_records(self):
        if not dist.is_initialized() or self.world_size == 1:
            return self.records
        all_records = None
        if self.rank == 0:
            all_records_list = [None] * self.world_size
            dist.gather_object(self.records, all_records_list, dst=0)
            all_records = [item for sublist in all_records_list for item in sublist]
        else:
            dist.gather_object(self.records, None, dst=0)
        return all_records

    def dump(self, sort_records: bool = False):
        """
        ã€ä¿®æ”¹åã€‘å°†å½“å‰ Rank çš„åŸå§‹è®¡æ—¶è®°å½•è¿½åŠ åˆ°æ—¥å¿—æ–‡ä»¶ä¸­ã€‚

        Args:
            sort_records (bool, optional): æ˜¯å¦åœ¨å†™å…¥å‰å¯¹è®°å½•è¿›è¡Œæ’åºã€‚
                                         é»˜è®¤ä¸º Falseï¼Œå³æŒ‰åŸå§‹æ‰§è¡Œé¡ºåºå†™å…¥ã€‚
                                         è®¾ç½®ä¸º True åˆ™æŒ‰ iteration å’Œ stage name æ’åºã€‚
        """
        if self.log_dir is None:
            return

        os.makedirs(self.log_dir, exist_ok=True)
        log_path = os.path.join(self.log_dir, f"{self.tag}_rank{self.rank}.log")

        # ã€ä¿®æ”¹ç‚¹ 1ã€‘: æ ¹æ® sort_records å‚æ•°å†³å®šæ˜¯å¦æ’åº
        if sort_records:
            records_to_write = sorted(
                self.records, key=lambda x: (x["iteration"], x["stage"])
            )
            sort_info = "(Sorted)"
        else:
            # é»˜è®¤æƒ…å†µä¸‹ï¼Œç›´æ¥ä½¿ç”¨åŸå§‹è®°å½•åˆ—è¡¨ï¼Œä¿ç•™æ‰§è¡Œé¡ºåº
            records_to_write = self.records
            sort_info = "(Execution Order)"

        with open(log_path, "a") as f:
            f.write(
                f"\n==================== DUMP {sort_info} (Up to Iteration {self.current_iteration}) ====================\n"
            )

            # ã€ä¿®æ”¹ç‚¹ 2ã€‘: éå†å¤„ç†åçš„åˆ—è¡¨
            for r in records_to_write:
                # ç¡®ä¿ cuda_duration_ms å­˜åœ¨ï¼Œæä¾›ä¸€ä¸ªé»˜è®¤å€¼ä»¥é¿å…é”™è¯¯
                cuda_time = r.get("cuda_duration_ms")
                cuda_str = (
                    f"{cuda_time:>8.3f}ms" if cuda_time is not None else "N/A".rjust(8)
                )

                f.write(
                    f"[Iter {r['iteration']}][Rank {r['rank']}] Stage: {r['stage']:<30} | "
                    f"CPU: {r['cpu_duration_ms']:>8.3f}ms, CUDA: {cuda_str}\n"
                )


                
    def generate_report(self, stage_pattern, output_filename, iteration_filter=None):
        """
        ã€æœ€ç»ˆæ­£ç¡®ç‰ˆ V2ã€‘ç”Ÿæˆè¯¦ç»†çš„æ€§èƒ½åˆ†ææŠ¥å‘Šï¼Œå¹¶ä¿å­˜åˆ°æ–‡ä»¶ã€‚
        æ­¤ç‰ˆæœ¬ä¼šèšåˆæ‰€æœ‰ rank çš„æ•°æ®ï¼ŒæŒ‰ç‹¬ç«‹çš„ Stage Name è¿›è¡Œç»Ÿä¸€ç»Ÿè®¡ã€‚
        """
        # _gather_records() åº”è¯¥è¿”å›ä¸€ä¸ªåŒ…å«æ‰€æœ‰ rank è®°å½•çš„åˆ—è¡¨
        all_records = self._gather_records()

        if self.rank == 0:
            # 1. ç­›é€‰ç¬¦åˆæ¡ä»¶çš„è®°å½• (é€»è¾‘ä¸å˜)
            pattern = re.compile(stage_pattern)
            filtered_records = [
                r
                for r in all_records
                if pattern.match(r["stage"])
                and (iteration_filter is None or iteration_filter(r["iteration"]))
            ]

            if not filtered_records:
                self.logger.warning(
                    f"No records found for pattern '{stage_pattern}' to generate report."
                )
                return {}

            # --- æ ¸å¿ƒä¿®æ”¹åŒºåŸŸ ---

            # 2. æŒ‰ stage_name å¯¹æ‰€æœ‰ rank çš„æ•°æ®è¿›è¡Œåˆ†ç»„
            # æ–°é€»è¾‘ï¼šå°†æ‰€æœ‰ rank ä¸­ name ç›¸åŒçš„ stage èšåˆåœ¨ä¸€èµ·
            grouped_data = defaultdict(list)
            for r in filtered_records:
                # ä¸å†å…³å¿ƒ r["rank"]ï¼Œåªè¦ stage name ç›¸åŒï¼Œå°±èšåˆ
                if r["cuda_duration_ms"] is not None:
                    grouped_data[r["stage"]].append(r["cuda_duration_ms"])

            # 3. è®¡ç®—æ¯ä¸ªèšåˆå stage çš„ç»Ÿè®¡æ•°æ®
            report_data = {} # ä¸å†éœ€è¦æŒ‰ rank åˆ†ç»„
            for stage_name, durations in grouped_data.items():
                # æ¯ä¸ª stage_name éƒ½æ˜¯ä¸€ä¸ªç‹¬ç«‹çš„æ¡ç›®ï¼Œå…¶ durations æ˜¯æ¥è‡ªæ‰€æœ‰ rank çš„æ•°æ®åˆ—è¡¨
                report_data[stage_name] = {
                    "count": len(durations),
                    "mean": np.mean(durations),
                    "median": np.median(durations),
                    "std": np.std(durations),
                    "min": np.min(durations),
                    "max": np.max(durations),
                }
            
            # --- ä¿®æ”¹ç»“æŸ ---

            # 4. ç”Ÿæˆæ ¼å¼åŒ–çš„æŠ¥å‘Šå­—ç¬¦ä¸² (ç°åœ¨åªæœ‰ä¸€ä¸ªèšåˆåçš„æ€»è¡¨)
            report_string = ""
            report_header = (
                f"--- ğŸ“Š Aggregated Performance Report (All Ranks) ---\n"
                f"Pattern: '{stage_pattern}'\n"
                f"Filename: {output_filename}\n"
                f"{'-'*80}\n"
            )
            report_string += report_header

            # ä¸å†éœ€è¦ for rank_id in ... çš„å¾ªç¯
            report_string += f"\n[Aggregated Statistics]\n"
            report_string += f" {'STAGE':<60} {'COUNT':<7} {'MEAN (ms)':<12} {'MEDIAN (ms)':<13} {'STD (ms)':<12}\n"
            report_string += f" {'-'*59} {'-'*6} {'-'*11} {'-'*12} {'-'*11}\n"

            # è¡¨æ ¼å†…å®¹
            for stage_name in sorted(report_data.keys()):
                stats = report_data[stage_name]
                report_string += (
                    f" {stage_name:<60} {stats['count']:<7} "
                    f"{stats['mean']:<12.3f} {stats['median']:<13.3f} {stats['std']:<12.3f}\n"
                )

            # 5. æ‰“å°åˆ°æ§åˆ¶å° (æ— éœ€æ”¹åŠ¨)
            print(report_string)

            # 6. ä¿å­˜åˆ°æ–‡ä»¶ (æ— éœ€æ”¹åŠ¨)
            if self.log_dir:
                log_path = os.path.join(self.log_dir, output_filename)
                # os.makedirs(self.log_dir, exist_ok=True)
                # log_path = os.path.join(self.log_dir, output_filename)
                try:
                    with open(log_path, "w") as f:
                        f.write(report_string)
                    self.logger.info(f"Report successfully saved to '{log_path}'")
                except IOError as e:
                    self.logger.error(f"Failed to save report to '{log_path}': {e}")
            else:
                self.logger.warning("log_dir not set, cannot save report file.")

            return report_data

        return None
   
    def generate_csv(
        self, report_data: dict, csv_filename: str = "suffix_median_report.csv"
    ):
        """
        ä» generate_report çš„ç»“æœä¸­æå–åç¼€å‚æ•° bs/f/h/w/sp å’Œä¸­ä½æ•°ï¼Œå¹¶ä¿å­˜ä¸º CSV æ–‡ä»¶ã€‚

        Args:
            report_data (dict): generate_report è¿”å›çš„å­—å…¸ï¼Œç»“æ„ä¸º report_data[rank][suffix]ã€‚
            csv_filename (str): è¦ä¿å­˜çš„ CSV æ–‡ä»¶åï¼ˆé»˜è®¤ä¸º suffix_median_report.csvï¼‰
        """
        import csv

        if self.rank != 0:
            return  # åªåœ¨ Rank 0 ä¸Šæ‰§è¡Œ

        if self.log_dir is None:
            self.logger.warning("log_dir not set, cannot save CSV file.")
            return

        csv_path = os.path.join(self.log_dir, csv_filename)

        try:
            with open(csv_path, "w", newline="") as csvfile:
                writer = csv.writer(csvfile)
                writer.writerow(["bs", "f", "h", "w", "sp", "median"])  # è¡¨å¤´

                for suffix, stats in report_data[0].items():
                    match = re.match(
                        r"bs_(\d+)_f_(\d+)_h_(\d+)_w_(\d+)_sp(\d+)", suffix
                    )
                    if match:
                        bs, f, h, w, sp = match.groups()
                        median = stats["median"]
                        writer.writerow([bs, f, h, w, sp, median])

            self.logger.info(f"CSV report successfully saved to '{csv_path}'")

        except Exception as e:
            self.logger.error(f"Failed to save CSV report to '{csv_path}': {e}")



class NoOpMyTimer:
    """
    ä¸€ä¸ªä¸ MyTimer æ¥å£å…¼å®¹çš„ä¼ªè®¡æ—¶å™¨ã€‚
    å®ƒçš„æ‰€æœ‰æ–¹æ³•éƒ½æ˜¯ç©ºæ“ä½œï¼Œç”¨äºåœ¨ç¦ç”¨æ€§èƒ½åˆ†ææ—¶ä½œä¸º MyTimer çš„â€œæ›¿èº«â€ã€‚
    ä½¿ç”¨ *args å’Œ **kwargs ç¡®ä¿å®ƒèƒ½æ¥æ”¶ä»»ä½•å‚æ•°è€Œä¸ä¼šæŠ¥é”™ã€‚
    """
    def __init__(self, *args, **kwargs):
        pass

    def set_logger(self, logger_instance: logging.Logger):
        pass

    def start(self, stage_name: str, *args, **kwargs):
        pass

    def stop(self, stage_name: str, *args, **kwargs):
        pass

    def synchronize_and_log(self):
        pass

    def step(self):
        """synchronize_and_log çš„åˆ«å"""
        pass

    def next_iteration(self):
        pass

    def dump(self, *args, **kwargs):
        pass

    def generate_report(self, *args, **kwargs):
        # çœŸå®çš„æ–¹æ³•åœ¨ rank 0 ä¸Šè¿”å› dictï¼Œé rank 0 è¿”å› None
        # è¿™é‡Œç›´æ¥è¿”å› None ä»¥ä¿æŒè¡Œä¸ºä¸€è‡´
        return None

    def generate_csv(self, *args, **kwargs):
        pass
    
    # ä¹Ÿå¯ä»¥æ·»åŠ å…¶ä»–å…¬å…±æ–¹æ³•ï¼Œå¦‚ register_stage, disable_cuda_time ç­‰
    def register_stage(self, *args, **kwargs):
        pass

    def disable_cuda_time(self, *args, **kwargs):
        pass

    # ä¸Šä¸‹æ–‡ç®¡ç†å™¨ä¹Ÿéœ€è¦å®ç°
    @contextmanager
    def time_stage(self, stage_name: str, *args, **kwargs):
        try:
            yield
        finally:
            pass # æ— éœ€æ‰§è¡Œä»»ä½•æ“ä½œ

    def register_stage(self, *args, **kwargs):
            """ç©ºæ“ä½œçš„æ³¨å†Œæ–¹æ³•ï¼Œæ¥æ”¶ä»»ä½•å‚æ•°ä½†ä»€ä¹ˆä¹Ÿä¸åšã€‚"""
            pass



# SPMDä¸‹ä½¿ç”¨çš„å…¨å±€å®ä¾‹        
PROFILING_ENABLED = os.environ.get("ENABLE_TIMER", "0") == "1"
if PROFILING_ENABLED:
    global_timer = MyTimer()
else: 
    global_timer = NoOpMyTimer()


def get_global_timer():
    return global_timer

# (åœ¨ my_utils.init_utils.py ä¸­)

import os
import logging
import torch.distributed as dist
from my_utils.logger import GlobalLogger, get_global_logger
from my_utils.memory_snapshot import global_snapshotter
def setup_logging_and_timer(args, role_tag: str, use_cuda: bool, is_distributed: bool):
    """
    ä¸ºå½“å‰è¿›ç¨‹ (Worker æˆ– Driver) åˆå§‹åŒ– GlobalLogger å’Œ MyTimerã€‚
    
    è¿”å›:
        (logging.Logger, MyTimer/NoOpTimer): é…ç½®å¥½çš„ logger å’Œ timer å®ä¾‹ã€‚
    """
    
    # --- 1. é…ç½® GlobalLogger ---
    logger_instance = GlobalLogger()
    
    if not logger_instance.is_configured:
        if is_distributed:
            # Worker è¿›ç¨‹: ä» torch.dist è·å– rank
            rank = dist.get_rank()
            world_size = dist.get_world_size()
        elif os.environ.get('LOCAL_RANK') is not None:
            # torchrun
            rank = int(os.environ['LOCAL_RANK'])
            world_size = int(os.environ.get('WORLD_SIZE', 1))
            print(f"Detected torchrun environment: LOCAL_RANK={rank}, WORLD_SIZE={world_size}")
        else:
            # Driver è¿›ç¨‹: æ€»æ˜¯ 0/1
            rank = 0
            world_size = 1
            
        if not hasattr(args, 'log_dir') or args.logdir is None:
            base_log_dir = "logs"
        else:
            base_log_dir = args.logdir

        # e.g., "logs/Critic" æˆ– "logs/Trainer_Driver"
        log_dir = os.path.join(base_log_dir, str(role_tag))
        
        logger_instance.setup(
            log_dir=log_dir,
            level=logging.INFO, # or args.log_level
            rank=rank,
            world_size=world_size,
            extra_log_label=str(role_tag)
        )
    
    logger = get_global_logger()
    logger.info(f"Logger for {role_tag} (Rank {rank} World_size {world_size}) configured.")
    timer = None
    # --- 2. é…ç½® MyTimer ---
    if hasattr(args, 'use_ray') and args.use_ray :
        if os.environ.get("ENABLE_TIMER", "0") == "1":
            logger.info(f"Performance Timer ENABLED for {role_tag}")
            
            timer = MyTimer(
                use_cuda=use_cuda,
                tag=str(role_tag),
                log_dir=log_dir,
                use_nvtx=False, 
                profile_memory=False
            )
            
            # [!!] æ³¨å…¥ Logger (å·²ä¿®å¤ Bug)

            
        else:
            timer = NoOpMyTimer()
        timer.set_logger(logger)
    
    global_timer.set_logger(logger)
    global_timer.use_cuda = use_cuda
    global_timer.tag = str(role_tag)
    # global_timer.log_dir = logger_instance.log_dir # (é‡ç”¨ logger çš„ log_dir)
    # global_timer.rank = logger_instance.rank
    
    # (æ¥è‡ªä½  V1 çš„ç¡¬ç¼–ç )
    global_timer.use_nvtx = False 
    global_timer.profile_memory = False
    
    # 3. æ£€æŸ¥ global_timer æ˜¯å“ªç§ç±»å‹å¹¶è®°å½•
    # (æˆ‘ä»¬é€šè¿‡æ£€æŸ¥å®ƒæ˜¯å¦æ˜¯ NoOpTimer æ¥åˆ¤æ–­ ENABLE_TIMER çš„çŠ¶æ€)
    if isinstance(global_timer, NoOpMyTimer):
        logger.info(f"Performance Timer is DISABLED for {role_tag}.")
    else:
        logger.info(f"Performance Timer ENABLED for {role_tag} (Rank {global_timer.rank}).")

    


    global_snapshotter.set_logger(logger=logger)
    
    return logger, timer
    


def print_cuda_memory_gb(step_name=""):
    """
    æ‰“å°å½“å‰è¿›ç¨‹ï¼ˆRankï¼‰çš„å·²åˆ†é…å’Œå·²ç¼“å­˜çš„ CUDA æ˜¾å­˜ã€‚
    å•ä½ä¸º GBã€‚
    """
    # ç¡®ä¿ CUDA å¯ç”¨ä¸”åˆ†å¸ƒå¼ç¯å¢ƒå·²åˆå§‹åŒ–
    if torch.cuda.is_available() and dist.is_initialized():
        rank = dist.get_rank()
        allocated = torch.cuda.memory_allocated() / 1024**3
        reserved = torch.cuda.memory_reserved() / 1024**3
        print(
            f"âœ… [Rank {rank}] [CUDA Memory] {step_name}: "
            f"Allocated: {allocated:.3f} GB, Reserved: {reserved:.3f} GB"
        )
    else:
        # å¦‚æœæ²¡æœ‰åˆ†å¸ƒå¼ç¯å¢ƒæˆ– CUDAï¼Œåªæ‰“å°æ™®é€šä¿¡æ¯
        print(f"âœ… {step_name}: CUDA not available or distributed not initialized.")




import threading
import traceback

class DebuggingEvent(threading.Event):
    """
    ä¸€ä¸ªå¢å¼ºçš„ Event ç±»ï¼Œç”¨äºè°ƒè¯•ã€‚
    å®ƒåœ¨åˆå§‹åŒ–æ—¶æ¥æ”¶ä¸€ä¸ª logger å¯¹è±¡ï¼Œå¹¶åœ¨ .set() æ–¹æ³•è¢«è°ƒç”¨æ—¶ï¼Œ
    ä½¿ç”¨è¯¥ logger è®°å½•å †æ ˆä¿¡æ¯ã€‚
    """
    def __init__(self, *args, logger=None, **kwargs):
        # è°ƒç”¨çˆ¶ç±»çš„æ„é€ å‡½æ•°
        super().__init__(*args, **kwargs)
        
        # ä¿å­˜ logger å¯¹è±¡ï¼Œå¦‚æœæœªæä¾›ï¼Œåˆ™åˆ›å»ºä¸€ä¸ªé»˜è®¤çš„ print logger
        if logger:
            self.logger = logger
        else:
            # Fallback: å¦‚æœæ²¡æœ‰æä¾› loggerï¼Œå°±é€€å›åˆ°æ‰“å°åˆ°æ§åˆ¶å°çš„è¡Œä¸º
            self.logger = logging.getLogger("DebuggingEvent")
            if not self.logger.handlers:
                self.logger.addHandler(logging.StreamHandler())
            self.logger.setLevel(logging.INFO)

    def set(self):
        # ä½¿ç”¨ StringIO æ¥æ•è·å †æ ˆä¿¡æ¯ï¼Œè€Œä¸æ˜¯ç›´æ¥æ‰“å°
        import io
        s = io.StringIO()
        traceback.print_stack(file=s)
        stack_info = s.getvalue()
        s.close()

        # ä½¿ç”¨æˆ‘ä»¬ä¿å­˜çš„ logger æ¥è®°å½•ä¿¡æ¯
        self.logger.info(
            f"\n{'='*30} [EVENT SET TRACE] {'='*30}\n"
            f">>> Event object {id(self)} is being set() by:\n"
            f"{stack_info}"
            f"{'='*80}"
        )
        
        # è°ƒç”¨çˆ¶ç±»çš„åŸå§‹ set æ–¹æ³•
        super().set()



def record_oom_threshold(failing_bs: int, failing_frame: int, step: int = 4):
    """
    å½“å‘ç”ŸOOMæ—¶ï¼Œè®°å½•ä¸‹å¯¹åº”batch sizeçš„å®‰å…¨å¸§æ•°ä¸Šé™ã€‚

    è¿™ä¸ªå‡½æ•°æ˜¯å¹‚ç­‰çš„ï¼š
    1. å¦‚æœæ–‡ä»¶ä¸å­˜åœ¨ï¼Œä¼šè‡ªåŠ¨åˆ›å»ºã€‚
    2. å¦‚æœè®°å½•å·²å­˜åœ¨ï¼Œåªæœ‰åœ¨æ–°çš„ä¸Šé™æ›´ä¸¥æ ¼ï¼ˆæ›´å°ï¼‰æ—¶æ‰ä¼šæ›´æ–°ã€‚

    Args:
        failing_bs (int): å¯¼è‡´OOMçš„batch sizeã€‚
        failing_frame (int): å¯¼è‡´OOMçš„å¸§æ•°ã€‚
        step (int): å¸§æ•°é€’å¢çš„æ­¥é•¿ï¼Œç”¨äºè®¡ç®—ä¸Šä¸€ä¸ªå®‰å…¨ç‚¹ã€‚
    """
    import json
    # from megatron.core import mpu
    sp = os.environ.get('sp')
    model = os.environ.get('model_type') 
    resolution = os.environ.get('resolution')

    threshold_file = f"oom_thresholds_{model}_{resolution}_sp{sp}.json"
    print(f"--- OOM Detected! Recording threshold for bs={failing_bs} ---")
    
    # æ ¹æ®å¤±è´¥çš„å¸§æ•°ï¼Œè®¡ç®—ä¸Šä¸€ä¸ªå·²çŸ¥çš„â€œå®‰å…¨â€ç‚¹
    # ä¾‹å¦‚ï¼šframe=73 å¤±è´¥äº†, ä¸Šé™åˆ™ä¸º 69 (73-4)
    new_max_frame = failing_frame - step
    
    # è¯»å–å·²æœ‰çš„é˜ˆå€¼æ–‡ä»¶ï¼Œå¦‚æœä¸å­˜åœ¨æˆ–ä¸ºç©ºåˆ™åˆ›å»ºä¸€ä¸ªæ–°çš„å­—å…¸
    thresholds = {}
    if os.path.exists(threshold_file):
        try:
            with open(threshold_file, 'r', encoding='utf-8') as f:
                content = f.read()
                if content: # ç¡®ä¿æ–‡ä»¶ä¸ä¸ºç©º
                    thresholds = json.loads(content)
        except (json.JSONDecodeError, FileNotFoundError):
            print(f"Warning: Could not read or parse '{threshold_file}'. Starting with empty thresholds.")
            thresholds = {} # å‡ºé”™æ—¶é‡ç½®
    
    # JSONçš„keyå¿…é¡»æ˜¯å­—ç¬¦ä¸²
    failing_bs_str = str(failing_bs)
    
    # è·å–å½“å‰bså·²è®°å½•çš„ä¸Šé™ï¼Œå¦‚æœä¸å­˜åœ¨åˆ™è®¾ä¸ºæ— ç©·å¤§
    current_max = thresholds.get(failing_bs_str, float('inf'))
    
    # åªæœ‰åœ¨æ–°çš„ä¸Šé™æ¯”æ—§çš„æ›´ä¸¥æ ¼ï¼ˆæ›´å°ï¼‰æ—¶æ‰æ›´æ–°
    if new_max_frame < current_max:
        print(f"Updating bs={failing_bs} max frame from {current_max} to {new_max_frame}")
        thresholds[failing_bs_str] = new_max_frame
        
        # å°†æ›´æ–°åçš„é˜ˆå€¼æ¼‚äº®åœ°å†™å›æ–‡ä»¶
        with open(threshold_file, 'w', encoding='utf-8') as f:
            json.dump(thresholds, f, indent=4)
            print(f"Successfully saved new thresholds to '{threshold_file}'.")
    else:
        print(f"New max frame ({new_max_frame}) is not stricter than existing ({current_max}). No update needed.")



def print_tensor_info(tensor: torch.Tensor, name: str = ""):
        """
        å°†ä¸€ä¸ª PyTorch Tensor çš„ Rank, Shape, Device, å’Œ Dtype æ‰“å°åœ¨åŒä¸€è¡Œã€‚

        Args:
            tensor (torch.Tensor): éœ€è¦æ£€æŸ¥çš„ PyTorch Tensor.
            name (str, optional): Tensor çš„åå­—ï¼Œç”¨äºåœ¨æ‰“å°æ—¶åŒºåˆ†. Defaults to "".
        """
        if not isinstance(tensor, torch.Tensor):
            print(f"æä¾›çš„è¾“å…¥ '{name}' ä¸æ˜¯ä¸€ä¸ªæœ‰æ•ˆçš„ PyTorch Tensorã€‚")
            return

        # ä½¿ç”¨ f-string å°†æ‰€æœ‰ä¿¡æ¯æ ¼å¼åŒ–åˆ°ä¸€è¡Œ
        # å¦‚æœæä¾›äº† nameï¼Œåˆ™åœ¨å‰é¢åŠ ä¸Š "name: "
        prefix = f"{name}: " if name else ""
        print(
            f"{prefix}"
            f"Rank {dist.get_rank()}, "
            f"Shape={tensor.shape}, "
            f"Device='{tensor.device}', "
            f"Dtype={tensor.dtype}"
        )


from tensordict import TensorDict # (å‡è®¾ DataProto.batch æ˜¯è¿™ä¸ªç±»å‹)


IS_ENABLED = os.environ.get("DEBUG_DATA_CONSISTENCY", "0") == "1"
CSUM_PREFIX = "_csum_"

def _get_checksum_for_slice(tensor_slice: torch.Tensor) -> float:
    """
    [V6] è®¡ç®— *å•ä¸ª* æ‰¹æ¬¡é¡¹ (slice) çš„æ ¡éªŒå’Œã€‚
    """
    if not IS_ENABLED: return 0.0
    try:
        # (ç¡®ä¿ .float() ä»¥é˜²æ­¢ BFloat16 ç­‰çš„ç²¾åº¦é—®é¢˜)
        return torch.sum(tensor_slice.cpu().float()).item()
    except Exception:
        return -1.0

class ChecksumUtils:
    
    @staticmethod
    def sign(payload: dict):
        """
        [åœ¨ *å‘é€* ç«¯è°ƒç”¨ - V6 é€åˆ‡ç‰‡ç‰ˆ]
        
        [!!] æ ¸å¿ƒä¿®æ”¹: 
        1. éå† payload ä¸­çš„æ‰€æœ‰å¼ é‡ã€‚
        2. éå†è¯¥å¼ é‡çš„ *æ¯ä¸€é¡¹* (e.g., 0 åˆ° batch_size-1)ã€‚
        3. ä¸º *æ¯ä¸€é¡¹* è®¡ç®—æ ¡éªŒå’Œã€‚
        4. å°† [csum0, csum1, ...] åˆ—è¡¨åŒ…è£…æˆä¸€ä¸ª [BS] å½¢çŠ¶çš„å¼ é‡ã€‚
        """
        if not IS_ENABLED:
            return

        checksums_to_add = {}
        for key, value in payload.items():
            if not isinstance(value, torch.Tensor):
                continue

            csum_key = f"{CSUM_PREFIX}{key}"
            
            # [!!] V6 å…³é”®ä¿®å¤: éå† Batch [!!]
            batch_size = value.shape[0]
            csum_list = []
            for i in range(batch_size):
                # (ä¸ºç¬¬ i ä¸ªåˆ‡ç‰‡è®¡ç®—æ ¡éªŒå’Œ)
                csum_list.append(_get_checksum_for_slice(value[i]))
            
            # (åˆ›å»º [BS] å½¢çŠ¶çš„å¼ é‡)
            checksums_to_add[csum_key] = torch.tensor(
                csum_list, 
                dtype=torch.float32, 
                device=value.device # (æˆ– .cpu(), ä½† .device åŒ¹é…æ›´å¥½)
            )
        
        # [!!] å…³é”®: *ä¿®æ”¹* payload [!!]
        payload.update(checksums_to_add)

    @staticmethod
    def verify(batch: TensorDict, logger: logging.Logger):
        """
        [åœ¨ *æ¥æ”¶* ç«¯è°ƒç”¨ - V6 é€åˆ‡ç‰‡ç‰ˆ]
        
        åœ¨ *å·²åˆ‡ç‰‡* çš„ 'TensorDict' (.batch) å†…éƒ¨æ¯”è¾ƒæ ¡éªŒå’Œã€‚
        """
        if not IS_ENABLED:
            return
            
        if not logger:
            rank = dist.get_rank() if dist.is_initialized() else 0
            print(f"[Rank {rank}] ChecksumUtils.verify: No logger provided, skipping.")
            return

        csum_keys = [k for k in batch.keys() if k.startswith(CSUM_PREFIX)]
        if not csum_keys:
            logger.info("[Checksum] No checksum tensors found in TensorDict.")
            return

        for csum_key in csum_keys:
            original_key = csum_key[len(CSUM_PREFIX):]
            
            # 2. è·å– *æ ¡éªŒå’Œå¼ é‡* (e.g., Shape [2])
            expected_csums_tensor = batch[csum_key]
            
            # 3. è·å– *æ•°æ®å¼ é‡* (e.g., Shape [2, F, C, H, W])
            received_tensors = batch.get(original_key)
            
            if received_tensors is None:
                logger.warning(f"[Checksum] Found '{csum_key}' but "
                                 f"missing '{original_key}' in TensorDict!")
                continue
            
            batch_size = received_tensors.shape[0]
            if expected_csums_tensor.shape[0] != batch_size:
                 logger.error(f"[Checksum] FAILED: Mismatched batch size for '{original_key}'. "
                                f"Data has {batch_size} but csum has {expected_csums_tensor.shape[0]}.")
                 continue

            # [!!] 4. V6 æ ¸å¿ƒé€»è¾‘: éå† *åˆ‡ç‰‡å* çš„ Batch [!!]
            for i in range(batch_size):
                
                tensor_slice = received_tensors[i] # (è·å–ç¬¬ i ä¸ªå¼ é‡ (BS=1))
                expected_csum = expected_csums_tensor[i].item() # (è·å–ç¬¬ i ä¸ªæ ¡éªŒå’Œ (float))
                
                try:
                    # é‡æ–°è®¡ç®— *è¯¥åˆ‡ç‰‡* çš„æ ¡éªŒå’Œ
                    new_csum = _get_checksum_for_slice(tensor_slice)
                    
                    if not torch.allclose(torch.tensor(expected_csum), torch.tensor(new_csum)):
                         logger.error(
                            f"[!!] CHECKSUM MISMATCH (Key: {original_key}, Batch Index: {i}) [!!]\n"
                            f"  Sender (Generator)   Calculated: {expected_csum}\n"
                            f"  Receiver (Critic)  Re-calculated: {new_csum}"
                        )
                    else:
                        logger.info(
                            f"[Checksum OK] Key: {original_key} (Index: {i}, Sum: {new_csum})"
                        )
                except Exception as e:
                    logger.error(f"[Checksum] FAILED to verify '{original_key}': {e}")